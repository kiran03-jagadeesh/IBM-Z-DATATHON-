{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1d8fb4-c848-45a6-b7e3-3d642c7fe161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model Performance:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 1 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         3.0       0.00      0.00      0.00         2\n",
      "         4.0       0.67      0.67      0.67         3\n",
      "         5.0       0.40      0.33      0.36         6\n",
      "         6.0       0.50      0.50      0.50         2\n",
      "         7.0       0.75      0.50      0.60         6\n",
      "         8.0       0.33      0.50      0.40         2\n",
      "        10.0       0.00      0.00      0.00         3\n",
      "        11.0       0.50      0.50      0.50         2\n",
      "        12.0       1.00      0.33      0.50         3\n",
      "        13.0       0.00      0.00      0.00         2\n",
      "        14.0       0.17      0.17      0.17         6\n",
      "        15.0       0.14      0.14      0.14         7\n",
      "        16.0       0.25      0.29      0.27         7\n",
      "        17.0       0.00      0.00      0.00         7\n",
      "        18.0       0.30      0.60      0.40         5\n",
      "        19.0       0.00      0.00      0.00         1\n",
      "        20.0       0.29      0.29      0.29         7\n",
      "        21.0       0.00      0.00      0.00         7\n",
      "        22.0       0.57      0.80      0.67        35\n",
      "        23.0       0.33      0.11      0.17        18\n",
      "        24.0       0.40      0.33      0.36        24\n",
      "        25.0       0.53      0.70      0.60        27\n",
      "        26.0       0.00      0.00      0.00         2\n",
      "        27.0       0.00      0.00      0.00         1\n",
      "        28.0       0.68      0.75      0.71        59\n",
      "        29.0       0.69      0.73      0.71        83\n",
      "        30.0       0.63      0.66      0.65       225\n",
      "        31.0       0.43      0.39      0.41       214\n",
      "        32.0       0.67      0.74      0.70       732\n",
      "        33.0       0.63      0.66      0.64       574\n",
      "        34.0       0.48      0.33      0.39       172\n",
      "        35.0       0.71      0.46      0.56       103\n",
      "        36.0       0.81      0.88      0.84       340\n",
      "        37.0       0.51      0.49      0.50        77\n",
      "        38.0       0.63      0.55      0.59        67\n",
      "        39.0       0.48      0.44      0.46        34\n",
      "        40.0       0.60      0.58      0.59        66\n",
      "        41.0       0.48      0.38      0.43        26\n",
      "        42.0       0.38      0.40      0.39        25\n",
      "        43.0       0.36      0.28      0.31        18\n",
      "        44.0       0.42      0.36      0.38        14\n",
      "        45.0       0.00      0.00      0.00         5\n",
      "        46.0       0.65      0.62      0.63        21\n",
      "        47.0       0.00      0.00      0.00         4\n",
      "        48.0       0.20      0.25      0.22         4\n",
      "        49.0       0.22      0.25      0.24         8\n",
      "        50.0       0.38      0.30      0.33        10\n",
      "        51.0       0.07      0.14      0.09         7\n",
      "        52.0       0.23      0.23      0.23        13\n",
      "        53.0       0.14      0.25      0.18         4\n",
      "        54.0       0.33      0.12      0.18         8\n",
      "        55.0       0.29      0.17      0.21        12\n",
      "        56.0       0.36      0.31      0.33        13\n",
      "        57.0       0.00      0.00      0.00         6\n",
      "        58.0       0.25      0.33      0.29         6\n",
      "        59.0       0.43      0.23      0.30        13\n",
      "        61.0       0.23      0.30      0.26        10\n",
      "        62.0       0.00      0.00      0.00         5\n",
      "        63.0       0.00      0.00      0.00         1\n",
      "        65.0       0.00      0.00      0.00         1\n",
      "        66.0       0.00      0.00      0.00         2\n",
      "        67.0       0.14      0.20      0.17         5\n",
      "        68.0       0.08      0.17      0.11         6\n",
      "        69.0       0.43      0.20      0.27        15\n",
      "        70.0       0.17      0.11      0.13         9\n",
      "        71.0       0.00      0.00      0.00         4\n",
      "        72.0       0.17      0.20      0.18         5\n",
      "        73.0       0.00      0.00      0.00         4\n",
      "        74.0       0.33      0.33      0.33        12\n",
      "        75.0       0.00      0.00      0.00         7\n",
      "        76.0       0.00      0.00      0.00         4\n",
      "        77.0       0.00      0.00      0.00         7\n",
      "        78.0       0.12      0.20      0.15         5\n",
      "        79.0       0.30      0.23      0.26        13\n",
      "        80.0       0.10      0.14      0.12         7\n",
      "        81.0       0.09      0.33      0.14         6\n",
      "        82.0       0.32      0.26      0.29        31\n",
      "        83.0       0.33      0.26      0.29        27\n",
      "        84.0       0.22      0.19      0.21        21\n",
      "        85.0       0.21      0.27      0.24        11\n",
      "        86.0       0.33      0.40      0.36        10\n",
      "        87.0       0.00      0.00      0.00         5\n",
      "        88.0       0.00      0.00      0.00         2\n",
      "        89.0       0.00      0.00      0.00         1\n",
      "        90.0       0.00      0.00      0.00         2\n",
      "        91.0       0.00      0.00      0.00         2\n",
      "        92.0       0.00      0.00      0.00         2\n",
      "        93.0       0.00      0.00      0.00         2\n",
      "        94.0       0.00      0.00      0.00         1\n",
      "        95.0       0.00      0.00      0.00         3\n",
      "        96.0       0.25      0.33      0.29         3\n",
      "        98.0       0.00      0.00      0.00         1\n",
      "        99.0       0.00      0.00      0.00         2\n",
      "       100.0       1.00      0.33      0.50         3\n",
      "       101.0       0.00      0.00      0.00         0\n",
      "       102.0       0.00      0.00      0.00         4\n",
      "       103.0       0.10      0.20      0.13         5\n",
      "       104.0       0.27      0.38      0.32        16\n",
      "       105.0       0.38      0.38      0.38        21\n",
      "       106.0       0.19      0.23      0.21        13\n",
      "       107.0       0.00      0.00      0.00         9\n",
      "       108.0       0.50      0.14      0.22         7\n",
      "       109.0       0.00      0.00      0.00         1\n",
      "       110.0       0.00      0.00      0.00         3\n",
      "       111.0       0.00      0.00      0.00         2\n",
      "       112.0       0.00      0.00      0.00         3\n",
      "       113.0       0.00      0.00      0.00         0\n",
      "       114.0       0.00      0.00      0.00         3\n",
      "       115.0       0.00      0.00      0.00         1\n",
      "       116.0       0.50      0.25      0.33         4\n",
      "       117.0       0.00      0.00      0.00         1\n",
      "       118.0       0.00      0.00      0.00         4\n",
      "       119.0       0.50      0.33      0.40         6\n",
      "       120.0       0.10      0.17      0.12         6\n",
      "       121.0       0.25      0.50      0.33         2\n",
      "       122.0       0.33      0.50      0.40         2\n",
      "       123.0       0.00      0.00      0.00         3\n",
      "       124.0       0.00      0.00      0.00         3\n",
      "       125.0       0.50      0.33      0.40         3\n",
      "       127.0       0.00      0.00      0.00         1\n",
      "       129.0       0.00      0.00      0.00         0\n",
      "       130.0       0.00      0.00      0.00         1\n",
      "       131.0       0.00      0.00      0.00         5\n",
      "       132.0       0.67      0.22      0.33         9\n",
      "       133.0       0.29      0.40      0.33         5\n",
      "       134.0       0.00      0.00      0.00         1\n",
      "       135.0       0.20      0.17      0.18         6\n",
      "       136.0       0.33      0.25      0.29         8\n",
      "       137.0       0.50      0.17      0.25         6\n",
      "       138.0       0.00      0.00      0.00         4\n",
      "       139.0       0.44      0.44      0.44         9\n",
      "       140.0       0.19      0.33      0.24         9\n",
      "       141.0       0.00      0.00      0.00         4\n",
      "       142.0       0.17      0.25      0.20         4\n",
      "       143.0       0.17      0.10      0.12        10\n",
      "       144.0       0.60      0.32      0.41        19\n",
      "       145.0       0.00      0.00      0.00         4\n",
      "       146.0       0.50      0.64      0.56        11\n",
      "       147.0       0.00      0.00      0.00         1\n",
      "       148.0       0.48      0.56      0.51        18\n",
      "       149.0       0.33      0.33      0.33        15\n",
      "       150.0       0.49      0.58      0.53        36\n",
      "       151.0       0.00      0.00      0.00        15\n",
      "       152.0       0.53      0.61      0.57        54\n",
      "       153.0       0.88      0.87      0.87       127\n",
      "       154.0       0.00      0.00      0.00         1\n",
      "       155.0       0.33      0.27      0.30        33\n",
      "       156.0       0.74      0.70      0.72       125\n",
      "       157.0       0.67      0.71      0.69        76\n",
      "       158.0       0.63      0.69      0.66       168\n",
      "       159.0       0.34      0.29      0.31       216\n",
      "       160.0       0.75      0.81      0.78       723\n",
      "       161.0       0.53      0.46      0.49       204\n",
      "       162.0       0.62      0.68      0.65       147\n",
      "       163.0       0.59      0.44      0.51        95\n",
      "       164.0       0.78      0.82      0.80       130\n",
      "       165.0       0.75      0.67      0.70        57\n",
      "       166.0       0.65      0.83      0.73        53\n",
      "       167.0       0.50      0.26      0.34        23\n",
      "       168.0       0.73      0.81      0.77        74\n",
      "       169.0       0.12      0.08      0.10        13\n",
      "       170.0       0.66      0.72      0.69        32\n",
      "       171.0       0.75      0.33      0.46         9\n",
      "       172.0       0.00      0.00      0.00         2\n",
      "       173.0       0.00      0.00      0.00         3\n",
      "       174.0       0.57      0.40      0.47        10\n",
      "       175.0       0.00      0.00      0.00         4\n",
      "       176.0       0.12      0.20      0.15        10\n",
      "       177.0       0.40      0.44      0.42        27\n",
      "       178.0       0.33      0.16      0.21        19\n",
      "       179.0       0.00      0.00      0.00         0\n",
      "       180.0       0.00      0.00      0.00         2\n",
      "       181.0       0.00      0.00      0.00         1\n",
      "       182.0       0.00      0.00      0.00         2\n",
      "       183.0       0.50      0.25      0.33         4\n",
      "       184.0       0.62      0.73      0.67        11\n",
      "       185.0       0.50      0.33      0.40         3\n",
      "       186.0       0.00      0.00      0.00         2\n",
      "       187.0       0.00      0.00      0.00         2\n",
      "       194.0       0.50      1.00      0.67         1\n",
      "       195.0       0.00      0.00      0.00         1\n",
      "       242.0       0.00      0.00      0.00         1\n",
      "       246.0       0.00      0.00      0.00         1\n",
      "       253.0       0.50      0.67      0.57         3\n",
      "       254.0       0.00      0.00      0.00         1\n",
      "       255.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.59      6200\n",
      "   macro avg       0.27      0.25      0.25      6200\n",
      "weighted avg       0.58      0.59      0.58      6200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model Performance:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 1 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         3.0       0.00      0.00      0.00         2\n",
      "         4.0       0.67      0.67      0.67         3\n",
      "         5.0       0.40      0.33      0.36         6\n",
      "         6.0       0.50      0.50      0.50         2\n",
      "         7.0       0.75      0.50      0.60         6\n",
      "         8.0       0.33      0.50      0.40         2\n",
      "        10.0       0.00      0.00      0.00         3\n",
      "        11.0       0.50      0.50      0.50         2\n",
      "        12.0       1.00      0.33      0.50         3\n",
      "        13.0       0.00      0.00      0.00         2\n",
      "        14.0       0.17      0.17      0.17         6\n",
      "        15.0       0.14      0.14      0.14         7\n",
      "        16.0       0.25      0.29      0.27         7\n",
      "        17.0       0.00      0.00      0.00         7\n",
      "        18.0       0.30      0.60      0.40         5\n",
      "        19.0       0.00      0.00      0.00         1\n",
      "        20.0       0.29      0.29      0.29         7\n",
      "        21.0       0.00      0.00      0.00         7\n",
      "        22.0       0.57      0.80      0.67        35\n",
      "        23.0       0.33      0.11      0.17        18\n",
      "        24.0       0.40      0.33      0.36        24\n",
      "        25.0       0.53      0.70      0.60        27\n",
      "        26.0       0.00      0.00      0.00         2\n",
      "        27.0       0.00      0.00      0.00         1\n",
      "        28.0       0.68      0.75      0.71        59\n",
      "        29.0       0.69      0.73      0.71        83\n",
      "        30.0       0.63      0.66      0.65       225\n",
      "        31.0       0.43      0.39      0.41       214\n",
      "        32.0       0.67      0.74      0.70       732\n",
      "        33.0       0.63      0.66      0.64       574\n",
      "        34.0       0.48      0.33      0.39       172\n",
      "        35.0       0.71      0.46      0.56       103\n",
      "        36.0       0.81      0.88      0.84       340\n",
      "        37.0       0.51      0.49      0.50        77\n",
      "        38.0       0.63      0.55      0.59        67\n",
      "        39.0       0.48      0.44      0.46        34\n",
      "        40.0       0.60      0.58      0.59        66\n",
      "        41.0       0.48      0.38      0.43        26\n",
      "        42.0       0.38      0.40      0.39        25\n",
      "        43.0       0.36      0.28      0.31        18\n",
      "        44.0       0.42      0.36      0.38        14\n",
      "        45.0       0.00      0.00      0.00         5\n",
      "        46.0       0.65      0.62      0.63        21\n",
      "        47.0       0.00      0.00      0.00         4\n",
      "        48.0       0.20      0.25      0.22         4\n",
      "        49.0       0.22      0.25      0.24         8\n",
      "        50.0       0.38      0.30      0.33        10\n",
      "        51.0       0.07      0.14      0.09         7\n",
      "        52.0       0.23      0.23      0.23        13\n",
      "        53.0       0.14      0.25      0.18         4\n",
      "        54.0       0.33      0.12      0.18         8\n",
      "        55.0       0.29      0.17      0.21        12\n",
      "        56.0       0.36      0.31      0.33        13\n",
      "        57.0       0.00      0.00      0.00         6\n",
      "        58.0       0.25      0.33      0.29         6\n",
      "        59.0       0.43      0.23      0.30        13\n",
      "        61.0       0.23      0.30      0.26        10\n",
      "        62.0       0.00      0.00      0.00         5\n",
      "        63.0       0.00      0.00      0.00         1\n",
      "        65.0       0.00      0.00      0.00         1\n",
      "        66.0       0.00      0.00      0.00         2\n",
      "        67.0       0.14      0.20      0.17         5\n",
      "        68.0       0.08      0.17      0.11         6\n",
      "        69.0       0.43      0.20      0.27        15\n",
      "        70.0       0.17      0.11      0.13         9\n",
      "        71.0       0.00      0.00      0.00         4\n",
      "        72.0       0.17      0.20      0.18         5\n",
      "        73.0       0.00      0.00      0.00         4\n",
      "        74.0       0.33      0.33      0.33        12\n",
      "        75.0       0.00      0.00      0.00         7\n",
      "        76.0       0.00      0.00      0.00         4\n",
      "        77.0       0.00      0.00      0.00         7\n",
      "        78.0       0.12      0.20      0.15         5\n",
      "        79.0       0.30      0.23      0.26        13\n",
      "        80.0       0.10      0.14      0.12         7\n",
      "        81.0       0.09      0.33      0.14         6\n",
      "        82.0       0.32      0.26      0.29        31\n",
      "        83.0       0.33      0.26      0.29        27\n",
      "        84.0       0.22      0.19      0.21        21\n",
      "        85.0       0.21      0.27      0.24        11\n",
      "        86.0       0.33      0.40      0.36        10\n",
      "        87.0       0.00      0.00      0.00         5\n",
      "        88.0       0.00      0.00      0.00         2\n",
      "        89.0       0.00      0.00      0.00         1\n",
      "        90.0       0.00      0.00      0.00         2\n",
      "        91.0       0.00      0.00      0.00         2\n",
      "        92.0       0.00      0.00      0.00         2\n",
      "        93.0       0.00      0.00      0.00         2\n",
      "        94.0       0.00      0.00      0.00         1\n",
      "        95.0       0.00      0.00      0.00         3\n",
      "        96.0       0.25      0.33      0.29         3\n",
      "        98.0       0.00      0.00      0.00         1\n",
      "        99.0       0.00      0.00      0.00         2\n",
      "       100.0       1.00      0.33      0.50         3\n",
      "       101.0       0.00      0.00      0.00         0\n",
      "       102.0       0.00      0.00      0.00         4\n",
      "       103.0       0.10      0.20      0.13         5\n",
      "       104.0       0.27      0.38      0.32        16\n",
      "       105.0       0.38      0.38      0.38        21\n",
      "       106.0       0.19      0.23      0.21        13\n",
      "       107.0       0.00      0.00      0.00         9\n",
      "       108.0       0.50      0.14      0.22         7\n",
      "       109.0       0.00      0.00      0.00         1\n",
      "       110.0       0.00      0.00      0.00         3\n",
      "       111.0       0.00      0.00      0.00         2\n",
      "       112.0       0.00      0.00      0.00         3\n",
      "       113.0       0.00      0.00      0.00         0\n",
      "       114.0       0.00      0.00      0.00         3\n",
      "       115.0       0.00      0.00      0.00         1\n",
      "       116.0       0.50      0.25      0.33         4\n",
      "       117.0       0.00      0.00      0.00         1\n",
      "       118.0       0.00      0.00      0.00         4\n",
      "       119.0       0.50      0.33      0.40         6\n",
      "       120.0       0.10      0.17      0.12         6\n",
      "       121.0       0.25      0.50      0.33         2\n",
      "       122.0       0.33      0.50      0.40         2\n",
      "       123.0       0.00      0.00      0.00         3\n",
      "       124.0       0.00      0.00      0.00         3\n",
      "       125.0       0.50      0.33      0.40         3\n",
      "       127.0       0.00      0.00      0.00         1\n",
      "       129.0       0.00      0.00      0.00         0\n",
      "       130.0       0.00      0.00      0.00         1\n",
      "       131.0       0.00      0.00      0.00         5\n",
      "       132.0       0.67      0.22      0.33         9\n",
      "       133.0       0.29      0.40      0.33         5\n",
      "       134.0       0.00      0.00      0.00         1\n",
      "       135.0       0.20      0.17      0.18         6\n",
      "       136.0       0.33      0.25      0.29         8\n",
      "       137.0       0.50      0.17      0.25         6\n",
      "       138.0       0.00      0.00      0.00         4\n",
      "       139.0       0.44      0.44      0.44         9\n",
      "       140.0       0.19      0.33      0.24         9\n",
      "       141.0       0.00      0.00      0.00         4\n",
      "       142.0       0.17      0.25      0.20         4\n",
      "       143.0       0.17      0.10      0.12        10\n",
      "       144.0       0.60      0.32      0.41        19\n",
      "       145.0       0.00      0.00      0.00         4\n",
      "       146.0       0.50      0.64      0.56        11\n",
      "       147.0       0.00      0.00      0.00         1\n",
      "       148.0       0.48      0.56      0.51        18\n",
      "       149.0       0.33      0.33      0.33        15\n",
      "       150.0       0.49      0.58      0.53        36\n",
      "       151.0       0.00      0.00      0.00        15\n",
      "       152.0       0.53      0.61      0.57        54\n",
      "       153.0       0.88      0.87      0.87       127\n",
      "       154.0       0.00      0.00      0.00         1\n",
      "       155.0       0.33      0.27      0.30        33\n",
      "       156.0       0.74      0.70      0.72       125\n",
      "       157.0       0.67      0.71      0.69        76\n",
      "       158.0       0.63      0.69      0.66       168\n",
      "       159.0       0.34      0.29      0.31       216\n",
      "       160.0       0.75      0.81      0.78       723\n",
      "       161.0       0.53      0.46      0.49       204\n",
      "       162.0       0.62      0.68      0.65       147\n",
      "       163.0       0.59      0.44      0.51        95\n",
      "       164.0       0.78      0.82      0.80       130\n",
      "       165.0       0.75      0.67      0.70        57\n",
      "       166.0       0.65      0.83      0.73        53\n",
      "       167.0       0.50      0.26      0.34        23\n",
      "       168.0       0.73      0.81      0.77        74\n",
      "       169.0       0.12      0.08      0.10        13\n",
      "       170.0       0.66      0.72      0.69        32\n",
      "       171.0       0.75      0.33      0.46         9\n",
      "       172.0       0.00      0.00      0.00         2\n",
      "       173.0       0.00      0.00      0.00         3\n",
      "       174.0       0.57      0.40      0.47        10\n",
      "       175.0       0.00      0.00      0.00         4\n",
      "       176.0       0.12      0.20      0.15        10\n",
      "       177.0       0.40      0.44      0.42        27\n",
      "       178.0       0.33      0.16      0.21        19\n",
      "       179.0       0.00      0.00      0.00         0\n",
      "       180.0       0.00      0.00      0.00         2\n",
      "       181.0       0.00      0.00      0.00         1\n",
      "       182.0       0.00      0.00      0.00         2\n",
      "       183.0       0.50      0.25      0.33         4\n",
      "       184.0       0.62      0.73      0.67        11\n",
      "       185.0       0.50      0.33      0.40         3\n",
      "       186.0       0.00      0.00      0.00         2\n",
      "       187.0       0.00      0.00      0.00         2\n",
      "       194.0       0.50      1.00      0.67         1\n",
      "       195.0       0.00      0.00      0.00         1\n",
      "       242.0       0.00      0.00      0.00         1\n",
      "       246.0       0.00      0.00      0.00         1\n",
      "       253.0       0.50      0.67      0.57         3\n",
      "       254.0       0.00      0.00      0.00         1\n",
      "       255.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.59      6200\n",
      "   macro avg       0.27      0.25      0.25      6200\n",
      "weighted avg       0.58      0.59      0.58      6200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2024-10-20 03:40:47.648729: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20315136 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 4s 5ms/step - loss: 3.8578 - accuracy: 0.1232 - val_loss: 3.5660 - val_accuracy: 0.1516\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 4s 7ms/step - loss: 3.5257 - accuracy: 0.1558 - val_loss: 3.4315 - val_accuracy: 0.1891\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 4s 7ms/step - loss: 3.2589 - accuracy: 0.2238 - val_loss: 3.0661 - val_accuracy: 0.2589\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 4s 7ms/step - loss: 2.9441 - accuracy: 0.2706 - val_loss: 2.8845 - val_accuracy: 0.2825\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 4s 6ms/step - loss: 2.8115 - accuracy: 0.2796 - val_loss: 2.8660 - val_accuracy: 0.2607\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 4s 6ms/step - loss: 2.7285 - accuracy: 0.2874 - val_loss: 2.7367 - val_accuracy: 0.3024\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 4s 7ms/step - loss: 2.6568 - accuracy: 0.2958 - val_loss: 2.6619 - val_accuracy: 0.3141\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 4s 7ms/step - loss: 2.6074 - accuracy: 0.3054 - val_loss: 2.6376 - val_accuracy: 0.3141\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 2.5673 - accuracy: 0.3094 - val_loss: 2.6338 - val_accuracy: 0.3131\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 5s 7ms/step - loss: 2.5299 - accuracy: 0.3171 - val_loss: 2.6041 - val_accuracy: 0.3095\n",
      "194/194 [==============================] - 1s 2ms/step\n",
      "\n",
      "LSTM\n",
      "\n",
      "LSTM Model Performance:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.00      0.00      0.00         6\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         6\n",
      "           8       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00         3\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         3\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.00      0.00      0.00         6\n",
      "          15       0.00      0.00      0.00         7\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00         7\n",
      "          18       0.00      0.00      0.00         5\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         7\n",
      "          21       0.00      0.00      0.00         7\n",
      "          22       0.00      0.00      0.00        35\n",
      "          23       0.00      0.00      0.00        18\n",
      "          24       0.00      0.00      0.00        24\n",
      "          25       0.00      0.00      0.00        27\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       0.00      0.00      0.00        59\n",
      "          29       0.00      0.00      0.00        83\n",
      "          30       0.19      0.11      0.14       225\n",
      "          31       0.00      0.00      0.00       214\n",
      "          32       0.30      0.98      0.46       732\n",
      "          33       0.00      0.00      0.00       574\n",
      "          34       0.00      0.00      0.00       172\n",
      "          35       0.00      0.00      0.00       103\n",
      "          36       0.38      0.78      0.51       340\n",
      "          37       0.00      0.00      0.00        77\n",
      "          38       0.00      0.00      0.00        67\n",
      "          39       0.00      0.00      0.00        34\n",
      "          40       0.00      0.00      0.00        66\n",
      "          41       0.00      0.00      0.00        26\n",
      "          42       0.00      0.00      0.00        25\n",
      "          43       0.00      0.00      0.00        18\n",
      "          44       0.00      0.00      0.00        14\n",
      "          45       0.00      0.00      0.00         5\n",
      "          46       0.00      0.00      0.00        21\n",
      "          47       0.00      0.00      0.00         4\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.00      0.00      0.00         8\n",
      "          50       0.00      0.00      0.00        10\n",
      "          51       0.00      0.00      0.00         7\n",
      "          52       0.00      0.00      0.00        13\n",
      "          53       0.00      0.00      0.00         4\n",
      "          54       0.00      0.00      0.00         8\n",
      "          55       0.00      0.00      0.00        12\n",
      "          56       0.00      0.00      0.00        13\n",
      "          57       0.00      0.00      0.00         6\n",
      "          58       0.00      0.00      0.00         6\n",
      "          59       0.00      0.00      0.00        13\n",
      "          61       0.00      0.00      0.00        10\n",
      "          62       0.00      0.00      0.00         5\n",
      "          63       0.00      0.00      0.00         1\n",
      "          65       0.00      0.00      0.00         1\n",
      "          66       0.00      0.00      0.00         2\n",
      "          67       0.00      0.00      0.00         5\n",
      "          68       0.00      0.00      0.00         6\n",
      "          69       0.00      0.00      0.00        15\n",
      "          70       0.00      0.00      0.00         9\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00         5\n",
      "          73       0.00      0.00      0.00         4\n",
      "          74       0.00      0.00      0.00        12\n",
      "          75       0.00      0.00      0.00         7\n",
      "          76       0.00      0.00      0.00         4\n",
      "          77       0.00      0.00      0.00         7\n",
      "          78       0.00      0.00      0.00         5\n",
      "          79       0.00      0.00      0.00        13\n",
      "          80       0.00      0.00      0.00         7\n",
      "          81       0.00      0.00      0.00         6\n",
      "          82       0.25      0.10      0.14        31\n",
      "          83       0.00      0.00      0.00        27\n",
      "          84       0.00      0.00      0.00        21\n",
      "          85       0.00      0.00      0.00        11\n",
      "          86       0.00      0.00      0.00        10\n",
      "          87       0.00      0.00      0.00         5\n",
      "          88       0.00      0.00      0.00         2\n",
      "          89       0.00      0.00      0.00         1\n",
      "          90       0.00      0.00      0.00         2\n",
      "          91       0.00      0.00      0.00         2\n",
      "          92       0.00      0.00      0.00         2\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       0.00      0.00      0.00         1\n",
      "          95       0.00      0.00      0.00         3\n",
      "          96       0.00      0.00      0.00         3\n",
      "          98       0.00      0.00      0.00         1\n",
      "          99       0.00      0.00      0.00         2\n",
      "         100       0.00      0.00      0.00         3\n",
      "         102       0.00      0.00      0.00         4\n",
      "         103       0.00      0.00      0.00         5\n",
      "         104       0.00      0.00      0.00        16\n",
      "         105       0.00      0.00      0.00        21\n",
      "         106       0.00      0.00      0.00        13\n",
      "         107       0.00      0.00      0.00         9\n",
      "         108       0.00      0.00      0.00         7\n",
      "         109       0.00      0.00      0.00         1\n",
      "         110       0.00      0.00      0.00         3\n",
      "         111       0.00      0.00      0.00         2\n",
      "         112       0.00      0.00      0.00         3\n",
      "         114       0.00      0.00      0.00         3\n",
      "         115       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         117       0.00      0.00      0.00         1\n",
      "         118       0.00      0.00      0.00         4\n",
      "         119       0.00      0.00      0.00         6\n",
      "         120       0.00      0.00      0.00         6\n",
      "         121       0.00      0.00      0.00         2\n",
      "         122       0.00      0.00      0.00         2\n",
      "         123       0.00      0.00      0.00         3\n",
      "         124       0.00      0.00      0.00         3\n",
      "         125       0.00      0.00      0.00         3\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         1\n",
      "         131       0.00      0.00      0.00         5\n",
      "         132       0.00      0.00      0.00         9\n",
      "         133       0.00      0.00      0.00         5\n",
      "         134       0.00      0.00      0.00         1\n",
      "         135       0.00      0.00      0.00         6\n",
      "         136       0.00      0.00      0.00         8\n",
      "         137       0.00      0.00      0.00         6\n",
      "         138       0.00      0.00      0.00         4\n",
      "         139       0.00      0.00      0.00         9\n",
      "         140       0.00      0.00      0.00         9\n",
      "         141       0.00      0.00      0.00         4\n",
      "         142       0.00      0.00      0.00         4\n",
      "         143       0.00      0.00      0.00        10\n",
      "         144       0.00      0.00      0.00        19\n",
      "         145       0.00      0.00      0.00         4\n",
      "         146       0.00      0.00      0.00        11\n",
      "         147       0.00      0.00      0.00         1\n",
      "         148       0.00      0.00      0.00        18\n",
      "         149       0.00      0.00      0.00        15\n",
      "         150       0.00      0.00      0.00        36\n",
      "         151       0.00      0.00      0.00        15\n",
      "         152       0.00      0.00      0.00        54\n",
      "         153       0.44      0.45      0.45       127\n",
      "         154       0.00      0.00      0.00         1\n",
      "         155       0.00      0.00      0.00        33\n",
      "         156       0.10      0.31      0.15       125\n",
      "         157       0.31      0.67      0.42        76\n",
      "         158       0.00      0.00      0.00       168\n",
      "         159       0.00      0.00      0.00       216\n",
      "         160       0.38      0.90      0.53       723\n",
      "         161       0.00      0.00      0.00       204\n",
      "         162       0.27      0.41      0.32       147\n",
      "         163       0.00      0.00      0.00        95\n",
      "         164       0.12      0.25      0.16       130\n",
      "         165       0.00      0.00      0.00        57\n",
      "         166       0.00      0.00      0.00        53\n",
      "         167       0.00      0.00      0.00        23\n",
      "         168       0.04      0.04      0.04        74\n",
      "         169       0.00      0.00      0.00        13\n",
      "         170       0.00      0.00      0.00        32\n",
      "         171       0.00      0.00      0.00         9\n",
      "         172       0.00      0.00      0.00         2\n",
      "         173       0.00      0.00      0.00         3\n",
      "         174       0.00      0.00      0.00        10\n",
      "         175       0.00      0.00      0.00         4\n",
      "         176       0.00      0.00      0.00        10\n",
      "         177       0.00      0.00      0.00        27\n",
      "         178       0.00      0.00      0.00        19\n",
      "         180       0.00      0.00      0.00         2\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       0.00      0.00      0.00         2\n",
      "         183       0.00      0.00      0.00         4\n",
      "         184       0.00      0.00      0.00        11\n",
      "         185       0.00      0.00      0.00         3\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.00      0.00      0.00         2\n",
      "         194       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         253       0.00      0.00      0.00         3\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.31      6200\n",
      "   macro avg       0.02      0.03      0.02      6200\n",
      "weighted avg       0.13      0.31      0.18      6200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant features\n",
    "    X = df[['abs_index']].values # Only include numeric 'abs_index' initially\n",
    "    \n",
    "    # Add radar data columns with handling for non-string values\n",
    "    for i in range(1, 5):\n",
    "        X = np.column_stack((X, df[f'unit1_radar{i}'].apply(lambda x: int(x.split('/')[-1].split('_')[1]) if isinstance(x, str) else 0)))\n",
    "    \n",
    "    # Convert timestamp to numeric representation (e.g., total seconds)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%H-%M-%S.%f').dt.time\n",
    "    df['timestamp'] = df['timestamp'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second + x.microsecond / 1e6)  # Convert to total seconds\n",
    "\n",
    "    X = np.column_stack((X, df['timestamp'].values)) # Add the numeric timestamp\n",
    "    \n",
    "    y = df['unit1_overall-beam'].values\n",
    "    return X, y\n",
    "\n",
    "# Load the data\n",
    "X, y = load_data('scenario37.csv')\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# KNN Model\n",
    "def train_knn(X_train, y_train, n_neighbors=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n",
    "\n",
    "# Train KNN model\n",
    "knn_model = train_knn(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate KNN model\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "print(\"KNN Model Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# LSTM Model\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=(input_shape[1], 1)),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reshape data for LSTM (samples, time steps, features)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# One-hot encode the target variable\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant features\n",
    "    X = df[['abs_index']].values # Only include numeric 'abs_index' initially\n",
    "    \n",
    "    # Add radar data columns with handling for non-string values\n",
    "    for i in range(1, 5):\n",
    "        X = np.column_stack((X, df[f'unit1_radar{i}'].apply(lambda x: int(x.split('/')[-1].split('_')[1]) if isinstance(x, str) else 0)))\n",
    "    \n",
    "    # Convert timestamp to numeric representation (e.g., total seconds)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%H-%M-%S.%f').dt.time\n",
    "    df['timestamp'] = df['timestamp'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second + x.microsecond / 1e6)  # Convert to total seconds\n",
    "\n",
    "    X = np.column_stack((X, df['timestamp'].values)) # Add the numeric timestamp\n",
    "    \n",
    "    y = df['unit1_overall-beam'].values\n",
    "    return X, y\n",
    "\n",
    "# Load the data\n",
    "X, y = load_data('scenario37.csv')\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# KNN Model\n",
    "def train_knn(X_train, y_train, n_neighbors=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n",
    "\n",
    "# Train KNN model\n",
    "knn_model = train_knn(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate KNN model\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "print(\"KNN Model Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# LSTM Model\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=(input_shape[1], 1)),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reshape data for LSTM (samples, time steps, features)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "\n",
    "# One-hot encode the target variable\n",
    "num_classes = int(y.max() + 1)  # Convert num_classes to an integer using int()\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)  # Specify num_classes in to_categorical\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)  # Specify num_classes in to_categorical\n",
    "\n",
    "\n",
    "# Create and train LSTM model\n",
    "lstm_model = create_lstm_model(X_train_lstm.shape, num_classes)\n",
    "history = lstm_model.fit(X_train_lstm, y_train_cat, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Evaluate LSTM model\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "y_pred_lstm_classes = np.argmax(y_pred_lstm, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(\"\\nLSTM\")\n",
    "y_pred_lstm_classes = np.argmax(y_pred_lstm, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(\"\\nLSTM Model Performance:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_lstm_classes))\n",
    "print(classification_report(y_test_classes, y_pred_lstm_classes))\n",
    "\n",
    "# Visualize training history (if needed)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96cc4c-b6ab-489a-a74c-9e6177ea3805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ac74f-be9a-4850-b4a9-ccaddaa52b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
